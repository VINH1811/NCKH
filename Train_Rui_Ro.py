import pandas as pd
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, classification_report

# Äá»c dá»¯ liá»‡u
file_path = "german_credit_data.csv"  # Thay tháº¿ báº±ng Ä‘Æ°á»ng dáº«n thá»±c táº¿
df = pd.read_csv(file_path)

# XÃ³a cá»™t khÃ´ng cáº§n thiáº¿t
if "Unnamed: 0" in df.columns:
    df = df.drop(columns=["Unnamed: 0"])

# XÃ¡c Ä‘á»‹nh cá»™t sá»‘ vÃ  cá»™t phÃ¢n loáº¡i
cot_phan_loai = ["Sex", "Housing", "Saving accounts", "Checking account", "Purpose"]
cot_so = ["Age", "Job", "Credit amount", "Duration"]

# Xá»­ lÃ½ giÃ¡ trá»‹ khuyáº¿t thiáº¿u
bo_sung_du_lieu_phan_loai = SimpleImputer(strategy="constant", fill_value="unknown")
bo_sung_du_lieu_so = SimpleImputer(strategy="median")

# Chuáº©n hÃ³a dá»¯ liá»‡u sá»‘
bo_chuan_hoa = MinMaxScaler()

# MÃ£ hÃ³a biáº¿n phÃ¢n loáº¡i
bien_doi_phan_loai = Pipeline([
    ("bo_sung", bo_sung_du_lieu_phan_loai),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

# XÃ¢y dá»±ng bá»™ biáº¿n Ä‘á»•i cho dá»¯ liá»‡u sá»‘
bien_doi_so = Pipeline([
    ("bo_sung", bo_sung_du_lieu_so),
    ("chuan_hoa", bo_chuan_hoa)
])

# Káº¿t há»£p xá»­ lÃ½ dá»¯ liá»‡u sá»‘ vÃ  phÃ¢n loáº¡i
xu_ly_truoc = ColumnTransformer([
    ("so", bien_doi_so, cot_so),
    ("phan_loai", bien_doi_phan_loai, cot_phan_loai)
])

# MÃ£ hÃ³a nhÃ£n má»¥c tiÃªu
ma_hoa_nhan = LabelEncoder()
df["Risk"] = ma_hoa_nhan.fit_transform(df["Risk"])

# Chia táº­p dá»¯ liá»‡u
X = df.drop(columns=["Risk"])
y = df["Risk"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Fit vÃ  transform dá»¯ liá»‡u
X_train = xu_ly_truoc.fit_transform(X_train)
X_test = xu_ly_truoc.transform(X_test)

# LÆ°u transformer vÃ  LabelEncoder Ä‘á»ƒ sá»­ dá»¥ng sau nÃ y
joblib.dump(xu_ly_truoc, "preprocessor.pkl")
joblib.dump(ma_hoa_nhan, "label_encoder.pkl")

# TÃ¬m tham sá»‘ tá»‘i Æ°u cho XGBoost
param_grid = {
    "n_estimators": [50, 100, 200],
    "learning_rate": [0.01, 0.1, 0.2],
    "max_depth": [3, 5, 7]
}

grid_search = GridSearchCV(XGBClassifier(eval_metric="logloss", random_state=42, use_label_encoder=False),
                           param_grid=param_grid, cv=3, scoring="accuracy", n_jobs=-1)
grid_search.fit(X_train, y_train)

print("ğŸ¯ TÃ¬m tháº¥y tham sá»‘ tá»‘i Æ°u:", grid_search.best_params_)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i tham sá»‘ tá»‘i Æ°u
mo_hinh = XGBClassifier(**grid_search.best_params_, eval_metric="logloss", random_state=42, use_label_encoder=False)
mo_hinh.fit(X_train, y_train)

# Dá»± Ä‘oÃ¡n trÃªn táº­p test
y_du_doan = mo_hinh.predict(X_test)

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
do_chinh_xac = accuracy_score(y_test, y_du_doan)
bao_cao = classification_report(y_test, y_du_doan, target_names=ma_hoa_nhan.classes_)

print("\nğŸ¯ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh:")
print(f"ğŸ¯ Äá»™ chÃ­nh xÃ¡c: {do_chinh_xac:.4f}")
print("ğŸ“Š BÃ¡o cÃ¡o phÃ¢n loáº¡i:\n", bao_cao)

# LÆ°u mÃ´ hÃ¬nh
joblib.dump(mo_hinh, "credit_risk_model.pkl")
print("ğŸ’¾ MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u thÃ nh cÃ´ng!")

# Kiá»ƒm tra mÃ´ hÃ¬nh sau khi lÆ°u
mo_hinh_tai = joblib.load("credit_risk_model.pkl")
du_doan_moi = mo_hinh_tai.predict(X_test)
print(f"âœ… Kiá»ƒm tra mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u: {accuracy_score(y_test, du_doan_moi):.4f}")
